{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Innovation Lab\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu using PyTorch 2.5.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from typing import Dict\n",
    "from collections import OrderedDict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from flwr.common import NDArrays, Scalar\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on {DEVICE} using PyTorch {torch.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section defines all the possible hyperparameters that can be used to configure the model. Just update the values to your desired configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset hyperparameters\n",
    "MAX_LEN = 256\n",
    "\n",
    "# Model hyperparameters\n",
    "EMBED_SIZE = 128\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHS = 10\n",
    "NUM_CLIENTS = 10\n",
    "\n",
    "# General hyperparameters\n",
    "RANDOM_STATE = 42\n",
    "VERBOSE = True\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "Next, we define the datastructure, load the dataset and perform the necessary pre-processing. We begin by defining the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords loaded successfully.\n",
      "Punkt tokenizer loaded successfully.\n",
      "Data loaders created successfully.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import ssl\n",
    "import os\n",
    "\n",
    "# Bypass SSL certificate verification\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "try:\n",
    "    # Load stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    print(\"Stopwords loaded successfully.\")\n",
    "\n",
    "    # Load punkt tokenizer\n",
    "    punkt = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    print(\"Punkt tokenizer loaded successfully.\")\n",
    "except LookupError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Ensure 'stopwords' and 'punkt' are correctly placed in the specified directory.\")\n",
    "\n",
    "# Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, maxlen):\n",
    "        self.texts = [text[:maxlen] for text in texts]\n",
    "        self.labels = labels\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = torch.tensor(self.texts[idx], dtype=torch.long)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return text, label\n",
    "\n",
    "def _collate_fn(batch, vocab):\n",
    "    texts, labels = zip(*batch)\n",
    "    texts_padded = pad_sequence(texts, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return texts_padded, labels\n",
    "\n",
    "def _remove_stopwords(text):\n",
    "    stop = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    return (\n",
    "        pd.Series(text)\n",
    "        .fillna(\"\")  # Replace NaN with empty string\n",
    "        .astype(str)\n",
    "        .str.lower()\n",
    "        .replace(r\"[^\\w\\s]\", \"\", regex=True)\n",
    "        .apply(nltk.word_tokenize)\n",
    "        .apply(\n",
    "            lambda sentence: \" \".join([word for word in sentence if word not in stop])\n",
    "        )\n",
    "    )\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, tokens):\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = []\n",
    "        self.add_tokens(tokens)\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token not in self.word_to_idx:\n",
    "                self.idx_to_word.append(token)\n",
    "                self.word_to_idx[token] = len(self.idx_to_word) - 1\n",
    "\n",
    "    def update(self, new_tokens):\n",
    "        self.add_tokens(new_tokens)\n",
    "\n",
    "    def __getitem__(self, token):\n",
    "        return self.word_to_idx.get(token, self.word_to_idx[\"<unk>\"])  # Handle unknown tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_word)\n",
    "\n",
    "    def add_special_tokens(self, tokens):\n",
    "        self.update(tokens)\n",
    "\n",
    "def load_dataset(file_path, maxlen):\n",
    "    # Load data from CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Ensure columns are as expected\n",
    "    assert 'statement' in df.columns and 'status' in df.columns, \"CSV must have 'statement' and 'status' columns.\"\n",
    "\n",
    "    # Preprocess text data\n",
    "    text = _remove_stopwords(df['statement'])\n",
    "    tokens = [nltk.word_tokenize(sentence) for sentence in text]\n",
    "\n",
    "    # Flatten the list of tokens for vocabulary creation\n",
    "    flat_tokens = [token for sentence in tokens for token in sentence]\n",
    "    vocab = Vocabulary(flat_tokens)\n",
    "\n",
    "    # Add special tokens\n",
    "    vocab.add_special_tokens([\"<pad>\", \"<unk>\"])\n",
    "\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(df['status'])\n",
    "\n",
    "    # Encode texts\n",
    "    encoded_texts = [[vocab[token] for token in sentence] for sentence in tokens]\n",
    "\n",
    "    # Apply maxlen to encoded texts\n",
    "    encoded_texts = [text[:maxlen] for text in encoded_texts]\n",
    "\n",
    "    encoded_df = pd.DataFrame(\n",
    "        {\n",
    "            \"text\": encoded_texts,\n",
    "            \"category\": labels,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return encoded_df, vocab, label_encoder\n",
    "\n",
    "\n",
    "def create_dataloader(text, labels, vocab, maxlen, shuffle=True):\n",
    "    dataset = CustomDataset(text.tolist(), labels.tolist(), maxlen)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=lambda x: _collate_fn(x, vocab),\n",
    "    )\n",
    "\n",
    "# Load data from 'sample.csv'\n",
    "MAX_LEN = 256  # Example maximum length, adjust as needed\n",
    "RANDOM_STATE = 42  # Set a seed for reproducibility\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "try:\n",
    "    df, vocab, label_encoder = load_dataset('Data.csv', MAX_LEN)\n",
    "    train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "        df[\"text\"], df[\"category\"], test_size=0.2, random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    train_loader = create_dataloader(train_texts, train_labels, vocab, MAX_LEN)\n",
    "    test_loader = create_dataloader(test_texts, test_labels, vocab, MAX_LEN, shuffle=False)\n",
    "    print(\"Data loaders created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterward, we define all the necessary methods to load and pre-process the data. Therefore, we performed tokenization, stopword removal, building up a vocabulary, and encoded the tokens into a numerical representation on our own. For this we used NLTK and scikit-learn respectively. Furthermore, we incorporated a max length into our feature to reduce further computational resources. Additionally, we added padding to the text-feature to unify it, as the input for the model. For this, we used pytorch. Lastly, we label encode the target feature with scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to call the `load_dataset` method with the desired max length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we split the dataset into training and testing sets, based on a 80:20 ratio and transform them into PyTorch DataLoader objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State-of-the-art baseline\n",
    "This section implements and evaluates a state-of-the-art bidirectional LSTM model for text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "First, we define the model itself with its necessary parameters. Afterward, we implement a method \n",
    "- to train the model based on provided dataset.\n",
    "- evaluate the model based on a provided dataset by computing the utility metrics accuracy, precision, recall and f1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22261</th>\n",
       "      <td>[789, 1662, 14468, 4097, 535, 387, 5695, 33, 1...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41400</th>\n",
       "      <td>[167, 3896, 178, 84, 138, 229, 216, 68895, 4005]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20065</th>\n",
       "      <td>[31447, 29387, 45698, 45699, 5044, 3521, 45700...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30036</th>\n",
       "      <td>[13219, 42, 167, 810, 3061, 42650, 1368, 1080,...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>[925, 193, 1215, 736]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  category\n",
       "22261  [789, 1662, 14468, 4097, 535, 387, 5695, 33, 1...         6\n",
       "41400   [167, 3896, 178, 84, 138, 229, 216, 68895, 4005]         2\n",
       "20065  [31447, 29387, 45698, 45699, 5044, 3521, 45700...         6\n",
       "30036  [13219, 42, 167, 810, 3061, 42650, 1368, 1080,...         5\n",
       "780                                [925, 193, 1215, 736]         3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    A bidirectional LSTM model for sequence classification.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of the vocabulary.\n",
    "        output_size (int): Number of output classes.\n",
    "        padding_idx (int): Padding index for the embedding layer.\n",
    "        embed_size (int, optional): Size of the embedding vectors (default: 128).\n",
    "        hidden_size (int, optional): Number of hidden units in the LSTM (default: 256).\n",
    "        num_layers (int, optional): Number of LSTM layers (default: 2).\n",
    "        dropout (float, optional): Dropout rate applied to LSTM outputs (default: 0.5).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, padding_idx, embed_size=128, hidden_size=256, num_layers=2, dropout=0.5):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=padding_idx)\n",
    "        self.rnn = nn.LSTM(\n",
    "            embed_size,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, _) = self.rnn(embedded)\n",
    "        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
    "        out = self.dropout(hidden)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, num_epochs, optimizer, device=DEVICE, verbose=False):\n",
    "    \"\"\"\n",
    "    Trains the model for a specified number of epochs.\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for texts, labels in train_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss / len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device=DEVICE, verbose=False):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test dataset. The function calculates and returns loss, accuracy, precision, recall\n",
    "    and F1 score for the model's predictions on the test data.\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "\n",
    "    total, correct = 0, 0\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in test_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    total_loss /= len(test_loader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=1)\n",
    "    recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Test Loss: {total_loss:.4f}, Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "        print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return total_loss, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "In this section, we train the model, based on `NUM_EPOCHS` specified. We however, evaluate the model after each epoch and save the results to a prior specified file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_centralised(model, train_loader, test_loader, epochs, lr, momentum=0.9, device=DEVICE, verbose=False):\n",
    "    \"\"\" \n",
    "    Trains and evaluates the model using centralized training.\n",
    "    \"\"\"\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    train_model(model, train_loader, epochs, optim, device)\n",
    "    loss, accuracy, precision, recall, f1 = evaluate_model(model, test_loader, device)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Accuracy: {accuracy:.4f}, Loss: {loss:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return accuracy, loss, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the file to save the results to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"results/results_baseline.csv\"\n",
    "os.makedirs(\"results\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the bidirectional LSTM, based on the provided hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BidirectionalLSTM(\n",
    "    vocab_size=len(vocab),\n",
    "    output_size= len(label_encoder.classes_),\n",
    "    padding_idx=vocab[\"<pad>\"],\n",
    "    embed_size=EMBED_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need to start the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [2:03:08<00:00, 7388.44s/it]\n",
      "100%|██████████| 1/1 [1:23:40<00:00, 5020.63s/it]\n",
      "100%|██████████| 1/1 [1:47:34<00:00, 6454.67s/it]\n",
      "100%|██████████| 1/1 [1:28:07<00:00, 5287.09s/it]\n",
      "100%|██████████| 1/1 [1:26:41<00:00, 5201.05s/it]\n",
      "100%|██████████| 1/1 [1:42:22<00:00, 6142.52s/it]\n",
      "100%|██████████| 1/1 [1:47:59<00:00, 6479.30s/it]\n",
      "100%|██████████| 1/1 [1:44:07<00:00, 6247.70s/it]\n",
      "100%|██████████| 1/1 [1:27:03<00:00, 5223.63s/it]\n",
      "100%|██████████| 1/1 [6:35:27<00:00, 23727.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1d 6h 58min 44s\n",
      "Wall time: 21h 51min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs_count = 0\n",
    "for epochs in range(NUM_EPOCHS):\n",
    "    epochs_count += 1\n",
    "    acc, loss, precision, recall, f1 = run_centralised(model, train_loader, test_loader, epochs=1, lr=LEARNING_RATE, verbose=False)\n",
    "    with open(file_path, \"a\") as file:\n",
    "        file.write(f\"{epochs_count},{acc},{loss},{precision},{recall},{f1}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Towards Federated LSTM\n",
    "This section implements and evaluates the model we want to federate. Futher we implement the server and client for the federation process and start a simulation to evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "First, we define the model itself with its necessary parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    A single-layer LSTM model for text classification.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of the vocabulary.\n",
    "        output_size (int): Number of output classes.\n",
    "        padding_idx (int): Padding index for the embedding layer.\n",
    "        embed_size (int, optional): Size of the embedding vectors (default: 128).\n",
    "        hidden_size (int, optional): Number of hidden units in the LSTM (default: 256).\n",
    "        num_layers (int, optional): Number of LSTM layers (default: 2).\n",
    "        dropout (float, optional): Dropout rate applied to LSTM outputs (default: 0.5).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        output_size,\n",
    "        padding_idx,\n",
    "        embed_size=128,\n",
    "        hidden_size=256,\n",
    "        num_layers=2,\n",
    "        dropout=0.5,\n",
    "    ):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=padding_idx)\n",
    "        self.rnn = nn.LSTM(\n",
    "            embed_size,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, _) = self.rnn(embedded)\n",
    "        hidden = hidden[-1]\n",
    "        out = self.dropout(hidden)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "To have a value to compare to, we train and evaluate the model based on `NUM_EPOCHS` specified. The reason for this is to see how well the model overall performs and furthermore have a upper bound for our federated model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define the file to save the results to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"results/results_no_fed.csv\"\n",
    "os.makedirs(\"results\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the uni-directional LSTM, based on the provided hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(\n",
    "    vocab_size=len(vocab),\n",
    "    output_size= len(label_encoder.classes_),\n",
    "    padding_idx=vocab[\"<pad>\"],\n",
    "    embed_size=EMBED_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the bidirectional LSTM, based on the provided hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [1:57:31<00:00, 7051.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2h 33s\n",
      "Wall time: 1h 58min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs_count = 0\n",
    "for epochs in range(1):\n",
    "    epochs_count += epochs\n",
    "    acc, loss, precision, recall, f1 = run_centralised(model, train_loader, test_loader, epochs=1, lr=LEARNING_RATE, verbose=False)\n",
    "    with open(file_path, \"a\") as file:\n",
    "       file.write(f\"{epochs_count},{acc},{loss},{precision},{recall},{f1}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Federated LSTM\n",
    "Now we deal with federating the model and data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition data\n",
    "We start by partitioning the data into `num_partitions`. This is crucial for further work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from collections import Counter\n",
    "\n",
    "def partition_dataset_stratified(train_loader, test_loader, vocab, num_partitions, batch_size, val_ratio=0.1):\n",
    "    \"\"\" \n",
    "    Partitions the training dataset into subsets with stratified sampling and creates DataLoaders.\n",
    "    Ensures label distribution is equal across all partitions while excluding rare classes.\n",
    "    \"\"\"\n",
    "    # Extract dataset from DataLoader\n",
    "    train_dataset = train_loader.dataset\n",
    "    test_dataset = test_loader.dataset\n",
    "\n",
    "    # Extract labels\n",
    "    all_labels = [train_dataset[i][1].item() for i in range(len(train_dataset))]\n",
    "\n",
    "    # Compute class distribution\n",
    "    class_counts = Counter(all_labels)\n",
    "    print(\"Class Distribution Before Filtering:\", class_counts)\n",
    "\n",
    "    # Identify rare classes to exclude (e.g., classes with <= 1 sample)\n",
    "    rare_classes = [cls for cls, count in class_counts.items() if count <= 1]\n",
    "    print(\"Rare Classes to Exclude:\", rare_classes)\n",
    "\n",
    "    # Filter out rare classes\n",
    "    filtered_indices = [i for i in range(len(train_dataset)) if train_dataset[i][1].item() not in rare_classes]\n",
    "    filtered_dataset = Subset(train_dataset, filtered_indices)\n",
    "\n",
    "    # Update labels after filtering\n",
    "    filtered_labels = [filtered_dataset[i][1].item() for i in range(len(filtered_dataset))]\n",
    "\n",
    "    # Stratified K-Fold for splitting dataset into partitions\n",
    "    skf = StratifiedKFold(n_splits=num_partitions, shuffle=True, random_state=2023)\n",
    "    partitions = list(skf.split(range(len(filtered_dataset)), filtered_labels))\n",
    "\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "\n",
    "    for train_idx, val_idx in partitions:\n",
    "        # Further split the training indices into training and validation sets\n",
    "        train_subset = Subset(filtered_dataset, train_idx)\n",
    "        val_subset = Subset(filtered_dataset, val_idx)\n",
    "        \n",
    "        # Create DataLoaders for train and validation subsets\n",
    "        trainloaders.append(DataLoader(train_subset, batch_size=batch_size, shuffle=True, \n",
    "                                       num_workers=0, collate_fn=lambda x: _collate_fn(x, vocab)))\n",
    "        valloaders.append(DataLoader(val_subset, batch_size=batch_size, shuffle=False, \n",
    "                                     num_workers=0, collate_fn=lambda x: _collate_fn(x, vocab)))\n",
    "\n",
    "    # Create DataLoader for the test set\n",
    "    testloader = DataLoader(test_dataset, batch_size=128, collate_fn=lambda x: _collate_fn(x, vocab))\n",
    "\n",
    "    return trainloaders, valloaders, testloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution Before Filtering: Counter({3: 13000, 2: 12366, 6: 8569, 0: 3108, 1: 2296, 5: 2140, 4: 954, 7: 1})\n",
      "Rare Classes to Exclude: [7]\n"
     ]
    }
   ],
   "source": [
    "trainloaders, valloaders, testloader = partition_dataset_stratified(train_loader, test_loader, vocab, NUM_CLIENTS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization\n",
    "We also added a small visualization of the distribution of classes within on partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 Label Distribution: (array([0, 1, 2, 3, 4, 5, 6], dtype=int64), array([ 2900,  2143, 11541, 12133,   891,  1998,  7998], dtype=int64))\n",
      "Client 1 Label Distribution: (array([0, 1, 2, 3, 4, 5, 6], dtype=int64), array([ 2900,  2143, 11541, 12133,   891,  1998,  7998], dtype=int64))\n",
      "Client 2 Label Distribution: (array([0, 1, 2, 3, 4, 5, 6], dtype=int64), array([ 2900,  2143, 11541, 12133,   891,  1998,  7998], dtype=int64))\n",
      "Client 3 Label Distribution: (array([0, 1, 2, 3, 4, 5, 6], dtype=int64), array([ 2901,  2142, 11541, 12134,   891,  1997,  7998], dtype=int64))\n",
      "Client 4 Label Distribution: (array([0, 1, 2, 3, 4, 5, 6], dtype=int64), array([ 2901,  2143, 11542, 12134,   890,  1997,  7997], dtype=int64))\n",
      "Client 5 Label Distribution: (array([0, 1, 2, 3, 4, 5, 6], dtype=int64), array([ 2901,  2143, 11542, 12134,   890,  1997,  7997], dtype=int64))\n",
      "Client 6 Label Distribution: (array([0, 1, 2, 3, 4, 5, 6], dtype=int64), array([ 2901,  2143, 11542, 12134,   890,  1997,  7997], dtype=int64))\n",
      "Client 7 Label Distribution: (array([0, 1, 2, 3, 4, 5, 6], dtype=int64), array([ 2901,  2143, 11542, 12134,   890,  1997,  7997], dtype=int64))\n",
      "Client 8 Label Distribution: (array([0, 1, 2, 3, 4, 5, 6], dtype=int64), array([ 2901,  2143, 11542, 12133,   890,  1997,  7998], dtype=int64))\n",
      "Client 9 Label Distribution: (array([0, 1, 2, 3, 4, 5, 6], dtype=int64), array([ 2901,  2143, 11542, 12133,   890,  1997,  7998], dtype=int64))\n",
      "Client 10 Label Distribution: (array([0, 1, 2, 3, 4, 5, 6], dtype=int64), array([ 2901,  2143, 11542, 12133,   890,  1997,  7998], dtype=int64))\n",
      "Client 11 Label Distribution: (array([0, 1, 2, 3, 4, 5, 6], dtype=int64), array([ 2901,  2143, 11542, 12133,   890,  1997,  7998], dtype=int64))\n",
      "Client 12 Label Distribution: (array([0, 1, 2, 3, 4, 5, 6], dtype=int64), array([ 2901,  2143, 11542, 12133,   890,  1997,  7998], dtype=int64))\n",
      "Client 13 Label Distribution: (array([0, 1, 2, 3, 4, 5, 6], dtype=int64), array([ 2901,  2143, 11541, 12133,   891,  1998,  7998], dtype=int64))\n",
      "Client 14 Label Distribution: (array([0, 1, 2, 3, 4, 5, 6], dtype=int64), array([ 2901,  2143, 11541, 12133,   891,  1998,  7998], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "for i, loader in enumerate(trainloaders):\n",
    "    labels = []\n",
    "    for _, batch_labels in loader:\n",
    "        labels.extend(batch_labels.numpy())\n",
    "    print(f\"Client {i} Label Distribution: {np.unique(labels, return_counts=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_partition = trainloaders[1].dataset\n",
    "partition_indices = train_partition.indices\n",
    "\n",
    "plt.hist([train_partition.dataset[idx][1].item() for idx in partition_indices], bins=len(label_encoder.classes_), edgecolor=\"black\")\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Labels: [0, 1, 2, 3, 4, 5, 6]\n",
      "Number of Unique Labels: 7\n"
     ]
    }
   ],
   "source": [
    "# Combine all labels into a single tensor\n",
    "all_labels = torch.tensor([label for _, label in train_partition.dataset])\n",
    "\n",
    "# Get unique labels\n",
    "unique_labels = torch.unique(all_labels)\n",
    "num_unique_labels = unique_labels.size(0)\n",
    "\n",
    "print(f\"Unique Labels: {unique_labels.tolist()}\")\n",
    "print(f\"Number of Unique Labels: {num_unique_labels}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Client\n",
    "Next, we implement the client based on the Flower library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client(fl.client.NumPyClient):\n",
    "    \"\"\"\n",
    "    A Federated Learning client for training and evaluating an LSTM model on the dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, output_size, padding_idx, embed_size, hidden_size, num_layers, dropout, device, learning_rate, trainloader, valloader) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "        self.device = device\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = LSTM(\n",
    "            vocab_size=vocab_size,\n",
    "            output_size= output_size,\n",
    "            padding_idx=padding_idx,\n",
    "            embed_size=embed_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        ).to(device)\n",
    "\n",
    "\n",
    "    def set_parameters(self, parameters):\n",
    "        \"\"\" \n",
    "        Sets the model parameters from the provided list.\n",
    "        \"\"\"\n",
    "        params_dict = zip(self.model.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "        self.model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        \"\"\"\n",
    "        Returns the model parameters as a list of numpy arrays.\n",
    "        \"\"\"\n",
    "        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        \"\"\"\n",
    "        Trains the model on the local training data with the given parameters.\n",
    "        \"\"\"\n",
    "        self.set_parameters(parameters)\n",
    "        optim = torch.optim.SGD(self.model.parameters(), lr=self.learning_rate, momentum=0.9)\n",
    "        train_model(model=self.model, train_loader=self.trainloader, num_epochs=1, optimizer=optim, device=self.device, verbose=False,)\n",
    "        return self.get_parameters({}), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        \"\"\"\n",
    "        Evaluates the model on the local validation data with the given parameters.\n",
    "        \"\"\"\n",
    "        self.set_parameters(parameters)\n",
    "        loss, accuracy, precision, recall, f1 = evaluate_model(self.model, self.valloader)\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And add some additional function to create and evaluate the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluate_fn(vocab_size, output_size, padding_idx, embed_size, hidden_size, num_layers, dropout, device, testloader):\n",
    "    \"\"\"\n",
    "    Creates an evaluation function for federated learning.\n",
    "    \"\"\"\n",
    "    def evaluate_fn(server_round, parameters, config):\n",
    "        model = LSTM(\n",
    "            vocab_size=vocab_size,\n",
    "            output_size= output_size,\n",
    "            padding_idx=padding_idx,\n",
    "            embed_size=embed_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        ).to(device)\n",
    "\n",
    "        params_dict = zip(model.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "        loss, accuracy, precision, recall, f1 = evaluate_model(\n",
    "            model, testloader\n",
    "        )\n",
    "        return loss, {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "    return evaluate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_client_fn(vocab_size, output_size, padding_idx, embed_size, hidden_size, num_layers, dropout, device, learning_rate, trainloaders, valloaders):\n",
    "    \"\"\"\n",
    "    Creates a client function for federated learning.\n",
    "    \"\"\"\n",
    "    def client_fn(cid: str):\n",
    "        return Client(vocab_size, output_size, padding_idx, embed_size, hidden_size, num_layers, dropout, device, learning_rate, trainloader=trainloaders[int(cid)], valloader=valloaders[int(cid)]).to_client()\n",
    "\n",
    "    return client_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Server\n",
    "Lastly, we configure the aggregating strategy for the server and define the number of clients via the `NUM_CLIENTS` parameter. We further use all models to train and evaluate on all models available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=0.5, \n",
    "    fraction_evaluate=1,  \n",
    "    min_available_clients=NUM_CLIENTS, \n",
    "    evaluate_fn=get_evaluate_fn(len(vocab), len(label_encoder.classes_), vocab[\"<pad>\"], EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT, DEVICE, testloader)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_fn_callback = generate_client_fn(len(vocab), len(label_encoder.classes_), vocab[\"<pad>\"], EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT, DEVICE, LEARNING_RATE, trainloaders, valloaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "Next, we start the simulation of our federated ml model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Starting Flower simulation, config: num_rounds=3, no round_timeout\n",
      "2024-11-21 23:36:05,297\tINFO worker.py:1819 -- Started a local Ray instance.\n",
      "\u001b[92mINFO \u001b[0m:      Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'CPU': 8.0, 'object_store_memory': 249574195.0, 'node:127.0.0.1': 1.0, 'memory': 499148391.0, 'GPU': 1.0}\n",
      "\u001b[92mINFO \u001b[0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html\n",
      "\u001b[92mINFO \u001b[0m:      No `client_resources` specified. Using minimal resources for clients.\n",
      "\u001b[92mINFO \u001b[0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}\n",
      "\u001b[92mINFO \u001b[0m:      Flower VCE: Creating VirtualClientEngineActorPool with 8 actors\n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n",
      "\u001b[36m(ClientAppActor pid=23220)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
      "\u001b[36m(ClientAppActor pid=23220)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=23220)\u001b[0m             This is a deprecated feature. It will be removed\n",
      "\u001b[36m(ClientAppActor pid=23220)\u001b[0m             entirely in future versions of Flower.\n",
      "\u001b[36m(ClientAppActor pid=23220)\u001b[0m         \n",
      "\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      initial parameters (loss, other metrics): 2.078700240836086, {'accuracy': 0.023187859364690356, 'precision': 0.6115329331426771, 'recall': 0.023187859364690356, 'f1': 0.0010597467412486851}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 7)\n",
      "\u001b[36m(ClientAppActor pid=23220)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
      "\u001b[36m(ClientAppActor pid=23220)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=23220)\u001b[0m             This is a deprecated feature. It will be removed\n",
      "\u001b[36m(ClientAppActor pid=23220)\u001b[0m             entirely in future versions of Flower.\n",
      "\u001b[36m(ClientAppActor pid=23220)\u001b[0m         \n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[36m(ClientAppActor pid=35068)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
      "\u001b[36m(ClientAppActor pid=35068)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=35068)\u001b[0m             This is a deprecated feature. It will be removed\n",
      "\u001b[36m(ClientAppActor pid=35068)\u001b[0m             entirely in future versions of Flower.\n",
      "\u001b[36m(ClientAppActor pid=35068)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=27656)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=27656)\u001b[0m         \n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[36m(ClientAppActor pid=27656)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
      "\u001b[36m(ClientAppActor pid=27656)\u001b[0m             This is a deprecated feature. It will be removed\n",
      "\u001b[36m(ClientAppActor pid=27656)\u001b[0m             entirely in future versions of Flower.\n",
      "100%|██████████| 1/1 [3:15:08<00:00, 11707.69s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [3:15:09<00:00, 11709.40s/it]\n",
      "100%|██████████| 1/1 [3:15:19<00:00, 11719.36s/it]\n",
      "100%|██████████| 1/1 [3:18:55<00:00, 11935.34s/it]\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 3 results and 0 failures\n",
      "\u001b[93mWARNING \u001b[0m:   No fit_metrics_aggregation_fn provided\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (1, 1.6327566037695092, {'accuracy': 0.31831463851446884, 'precision': 0.6710997920848002, 'recall': 0.31831463851446884, 'f1': 0.15694344409464597}, 12022.7637293)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 7 clients (out of 7)\n",
      "\u001b[36m(ClientAppActor pid=23220)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
      "\u001b[36m(ClientAppActor pid=23220)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=23220)\u001b[0m             This is a deprecated feature. It will be removed\n",
      "\u001b[36m(ClientAppActor pid=23220)\u001b[0m             entirely in future versions of Flower.\n",
      "\u001b[36m(ClientAppActor pid=23220)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=27656)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=27656)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=35068)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=35068)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=35068)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=35068)\u001b[0m             This is a deprecated feature. It will be removed\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=35068)\u001b[0m             entirely in future versions of Flower.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=25864)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=25864)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=25864)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
      "\u001b[36m(ClientAppActor pid=25864)\u001b[0m             This is a deprecated feature. It will be removed\n",
      "\u001b[36m(ClientAppActor pid=25864)\u001b[0m             entirely in future versions of Flower.\n",
      "\u001b[36m(ClientAppActor pid=27844)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
      "\u001b[36m(ClientAppActor pid=27844)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=27844)\u001b[0m             This is a deprecated feature. It will be removed\n",
      "\u001b[36m(ClientAppActor pid=27844)\u001b[0m             entirely in future versions of Flower.\n",
      "\u001b[36m(ClientAppActor pid=27844)\u001b[0m         \n"
     ]
    }
   ],
   "source": [
    "# Run the simulation\n",
    "history = fl.simulation.start_simulation(\n",
    "    client_fn=client_fn_callback,\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=NUM_EPOCHS),\n",
    "    strategy=strategy,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results\n",
    "Lastly, we save the results to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to results\\results_federated_15_clients.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "output_dir = \"results\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Saving the CSV file\n",
    "rows = []\n",
    "for i in range(len(history.metrics_centralized['accuracy'])):\n",
    "    loss = history.losses_centralized[i][1]\n",
    "    epoch = history.metrics_centralized['accuracy'][i][0]\n",
    "    accuracy = history.metrics_centralized['accuracy'][i][1]\n",
    "    precision = history.metrics_centralized['precision'][i][1]\n",
    "    recall = history.metrics_centralized['recall'][i][1]\n",
    "    f1 = history.metrics_centralized['f1'][i][1]\n",
    "    \n",
    "    rows.append({\n",
    "        'epoch_nr': epoch,\n",
    "        'accuracy': accuracy,\n",
    "        \"loss\": loss,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    })\n",
    "\n",
    "df_metrics = pd.DataFrame(rows)\n",
    "\n",
    "# Save the DataFrame to a CSV file in the results directory\n",
    "csv_file_path = os.path.join(output_dir, f\"results_federated_{NUM_CLIENTS}_clients.csv\")\n",
    "df_metrics.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Metrics saved to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to server_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_19684\\3074798946.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "The predicted class is: Normal\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import nltk\n",
    "\n",
    "# Define the function to save the server model\n",
    "def save_model(model, path=\"server_model.pth\"):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "# Function to preprocess a custom input\n",
    "def preprocess_input(text, vocab, maxlen):\n",
    "    # Remove stopwords and tokenize\n",
    "    stop = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    text = (\n",
    "        pd.Series([text])\n",
    "        .str.lower()\n",
    "        .replace(r\"[^\\w\\s]\", \"\", regex=True)\n",
    "        .apply(nltk.word_tokenize)\n",
    "        .apply(lambda sentence: [word for word in sentence if word not in stop])\n",
    "    ).iloc[0]\n",
    "\n",
    "    # Convert tokens to indices\n",
    "    text_indices = [vocab[token] for token in text]\n",
    "    \n",
    "    # Pad the input to match the maximum length\n",
    "    text_padded = pad_sequence(\n",
    "        [torch.tensor(text_indices, dtype=torch.long)],\n",
    "        batch_first=True,\n",
    "        padding_value=vocab[\"<pad>\"],\n",
    "    )\n",
    "    \n",
    "    return text_padded\n",
    "\n",
    "# Define a function to classify custom input\n",
    "def classify_input(model, text, vocab, label_encoder, maxlen, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Preprocess the input\n",
    "        preprocessed_text = preprocess_input(text, vocab, maxlen).to(device)\n",
    "        \n",
    "        # Pass the input through the model\n",
    "        logits = model(preprocessed_text)\n",
    "        prediction = torch.argmax(logits, dim=1).item()\n",
    "        \n",
    "        # Decode the label\n",
    "        predicted_label = label_encoder.inverse_transform([prediction])[0]\n",
    "        \n",
    "        return predicted_label\n",
    "\n",
    "# After federated training completes\n",
    "# Save the server model\n",
    "save_model(model)\n",
    "\n",
    "# Load the saved model (optional, to ensure it's working correctly)\n",
    "model_path = \"server_model.pth\"\n",
    "model = LSTM(\n",
    "    vocab_size=len(vocab),\n",
    "    output_size=len(label_encoder.classes_),\n",
    "    padding_idx=vocab[\"<pad>\"],\n",
    "    embed_size=EMBED_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    ").to(DEVICE)\n",
    "model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# Take custom input from the user\n",
    "custom_text = input(\"Enter a text for classification: \")\n",
    "\n",
    "# Classify the custom input\n",
    "predicted_class = classify_input(model, custom_text, vocab, label_encoder, MAX_LEN, DEVICE)\n",
    "print(f\"The predicted class is: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "federated-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
